{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction.\n",
    "The CIFAR-10 dataset contains 60,000 color images of 32 x 32 pixels in 3 channels divided into 10 classes. Each class contains 6,000 images. The training set contains 50,000 images, while the test sets provides 10,000 images. This image taken from the CIFAR repository ( https://www.cs.toronto.edu/~kriz/cifar.html ). This is a classification problem with 10 classes(muti-label classification). We can take a view on this image for more comprehension of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is to recognize previously unseen images and assign them to one of the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's setting the models hyperparameters and others global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # The default batch size of keras.\n",
    "num_classes = 10  # Number of class for the dataset\n",
    "epochs = 100\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We load the data and split it between train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 87s 1us/step\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
    "# Count plot for training set\n",
    "sns.countplot(y_train.ravel(), ax=axs[0])\n",
    "axs[0].set_title('Distribution of training data')\n",
    "axs[0].set_xlabel('Classes')\n",
    "# Count plot for testing set\n",
    "sns.countplot(y_test.ravel(), ax=axs[1])\n",
    "axs[1].set_title('Distribution of Testing data')\n",
    "axs[1].set_xlabel('Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, each classe contain exacly 6000 examples( 5000 for training and 1000 for test).\n",
    "\n",
    "The graph above is very important for the training, for example if we had just 1000 samples of label 1 that will be a problem , the model will find difficulties to detect label 1\"less accuracy \", so that's not going to happend everything look fine. It's important to know the distribution of dataset behind different classes because the goodness of our model depend on it.\n",
    "\n",
    "Now let's doing some preprocessing.\n",
    "\n",
    "The output variable have 10 posible values. This is a multiclass classification problem. We need to encode these lables to one hot vectors (ex : \".bird\" -> [0,0,1,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data. Before we need to connvert data type to float for computation.\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Convert class vectors to binary class matrices. This is called one hot encoding.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model architecture Using ConVnets\n",
    "Now Let us define a suitable deep net.\n",
    "\n",
    "In the first stage, Our net will learn 32 convolutional filters, each of which with a 3 x 3 size. The output dimension is the same one of the input shape, so it will be 32 x 32 and activation is relu, which is a simple way of introducing non-linearity; folowed by another 32 convolutional filters, each of which with a 3 x 3 size and activation is also relu. After that we have a max-pooling operation with pool size 2 x 2 and a dropout at 25%.\n",
    "In the next stage in the deep pipeline, Our net will learn 64 convolutional filters, each of which with a 3 x 3 size. The output dimension is the same one of the input shape and activation is relu; folowed by another 64 convolutional filters, each of which with a 3 x 3 size and activation is also relu. After that we have a max-pooling operation with pool size 2 x 2 and a dropout at 25%.\n",
    "And the Final stage in the deep pipeline is a dense network with 512 units and relu activation followed by a dropout at 50% and by a softmax layer with 10 classes as output, one for each category.\n",
    "Now let us look at the code review for our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define the convnet\n",
    "model = Sequential()\n",
    "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# FLATTERN => DENSE => RELU => DROPOUT\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# a softmax classifier\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let us train the model.\n",
    "\n",
    "### 4. Model training\n",
    "Before making network ready for training we have to make sure to add below things:\n",
    "\n",
    "* **A loss function**: to measure how good the network is\n",
    "* **An optimizer**: to update network as it sees more data and reduce loss value\n",
    "* **Metrics**: to monitor performance of network\n",
    "Also note that for data augmentation:\n",
    "\n",
    "One of the most commun tehnique to avoid overfitting is data augmentation. And We know that overfitting is generaly occur when we don't have enough data for training the model. To avoid this overfitting problem, we need to expand artificially our dataset. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n",
    "\n",
    "Different data aumentation techniques are as follows: Cropping, Rotating, Scaling, Translating, Flipping, Adding Gaussian noise to input images, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 1.8150 - accuracy: 0.3349 - val_loss: 1.5763 - val_accuracy: 0.4273\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 1.4924 - accuracy: 0.4589 - val_loss: 1.3928 - val_accuracy: 0.4918\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 132s 85ms/step - loss: 1.3696 - accuracy: 0.5095 - val_loss: 1.3107 - val_accuracy: 0.5384\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 1.2755 - accuracy: 0.5479 - val_loss: 1.1745 - val_accuracy: 0.5833\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 1.1944 - accuracy: 0.5778 - val_loss: 1.1422 - val_accuracy: 0.5981\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 137s 88ms/step - loss: 1.1282 - accuracy: 0.6008 - val_loss: 1.0890 - val_accuracy: 0.6151\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 135s 86ms/step - loss: 1.0781 - accuracy: 0.6200 - val_loss: 1.0144 - val_accuracy: 0.6455\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 1.0312 - accuracy: 0.6383 - val_loss: 0.9510 - val_accuracy: 0.6721\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 134s 86ms/step - loss: 0.9917 - accuracy: 0.6509 - val_loss: 0.9214 - val_accuracy: 0.6806\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 137s 88ms/step - loss: 0.9590 - accuracy: 0.6624 - val_loss: 0.8966 - val_accuracy: 0.6867\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.9273 - accuracy: 0.6741 - val_loss: 0.8637 - val_accuracy: 0.7023\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 132s 85ms/step - loss: 0.9027 - accuracy: 0.6852 - val_loss: 0.8776 - val_accuracy: 0.6953\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.8772 - accuracy: 0.6934 - val_loss: 0.8174 - val_accuracy: 0.7155\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 134s 86ms/step - loss: 0.8567 - accuracy: 0.6994 - val_loss: 0.8424 - val_accuracy: 0.7123\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.8385 - accuracy: 0.7064 - val_loss: 0.8487 - val_accuracy: 0.7018\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.8162 - accuracy: 0.7147 - val_loss: 0.8325 - val_accuracy: 0.7170\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.8056 - accuracy: 0.7202 - val_loss: 0.7859 - val_accuracy: 0.7281\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 134s 86ms/step - loss: 0.7915 - accuracy: 0.7259 - val_loss: 0.7835 - val_accuracy: 0.7280\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 138s 88ms/step - loss: 0.7782 - accuracy: 0.7298 - val_loss: 0.7449 - val_accuracy: 0.7455\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.7659 - accuracy: 0.7350 - val_loss: 0.7505 - val_accuracy: 0.7403\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 134s 86ms/step - loss: 0.7546 - accuracy: 0.7394 - val_loss: 0.7268 - val_accuracy: 0.7524\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 134s 86ms/step - loss: 0.7506 - accuracy: 0.7424 - val_loss: 0.7165 - val_accuracy: 0.7557\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.7431 - accuracy: 0.7437 - val_loss: 0.7083 - val_accuracy: 0.7573\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 138s 88ms/step - loss: 0.7363 - accuracy: 0.7473 - val_loss: 0.7225 - val_accuracy: 0.7567\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.7280 - accuracy: 0.7512 - val_loss: 0.7148 - val_accuracy: 0.7586\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.7234 - accuracy: 0.7527 - val_loss: 0.7498 - val_accuracy: 0.7509\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 132s 85ms/step - loss: 0.7186 - accuracy: 0.7546 - val_loss: 0.7341 - val_accuracy: 0.7507\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 135s 86ms/step - loss: 0.7150 - accuracy: 0.7543 - val_loss: 0.6903 - val_accuracy: 0.7657\n",
      "Epoch 29/100\n",
      " 734/1563 [=============>................] - ETA: 1:06 - loss: 0.7085 - accuracy: 0.7587"
     ]
    }
   ],
   "source": [
    "history = None  # For recording the history of trainning process.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    history = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                    batch_size=batch_size),\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_test, y_test),\n",
    "                                    workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model.\n",
    "Training and validation curves.¶\n",
    "Let's see the training and validation process by the visualization of history of fitting. This allow us to quickly know if how our model fit our data (overfitting, underfitting, model convergence, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmodelhistory(history): \n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(history.history['accuracy']) \n",
    "    axs[0].plot(history.history['val_accuracy']) \n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy') \n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(history.history['loss']) \n",
    "    axs[1].plot(history.history['val_loss']) \n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss') \n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "plotmodelhistory(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after 60 epochs, the accuracy of our model doesn't really increase. But our model doesn't overffit.\n",
    "\n",
    "### Score trained model and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "# make prediction.\n",
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's investigate for errors.\n",
    "\n",
    "### Confusion matrix.\n",
    "Confusion matrix can be very helpfull to see your model drawbacks. We plot the confusion matrix of the validation results. For good vizualization of our confusion matrix, we have to define to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, ax=None, cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "    \"\"\"\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "    \n",
    "    ax.set_xlabel('Predicted Label') \n",
    "    ax.set_ylabel('True Label')\n",
    "    \n",
    "    return im, cbar\n",
    "\n",
    "def annotate_heatmap(im, data=None, fmt=\"d\", threshold=None):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "    \"\"\"\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            text = im.axes.text(j, i, format(data[i, j], fmt), horizontalalignment=\"center\",\n",
    "                                 color=\"white\" if data[i, j] > thresh else \"black\")\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(pred, axis=1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_test_errors = x_test[errors]\n",
    "\n",
    "cm = confusion_matrix(Y_true, Y_pred_classes) \n",
    "thresh = cm.max() / 2.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "im, cbar = heatmap(cm, labels, labels, ax=ax,\n",
    "                   cmap=plt.cm.Blues, cbarlabel=\"count of predictions\")\n",
    "texts = annotate_heatmap(im, data=cm, threshold=thresh)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification report¶\n",
    "This will allow us to evaluate the model with other metrics (Precision, Recall, F1 score, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_true, Y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 5\n",
    "C = 5\n",
    "fig, axes = plt.subplots(R, C, figsize=(12,12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, R*C):\n",
    "    axes[i].imshow(x_test[i])\n",
    "    axes[i].set_title(\"True: %s \\nPredict: %s\" % (labels[Y_true[i]], labels[Y_pred_classes[i]]))\n",
    "    axes[i].axis('off')\n",
    "    plt.subplots_adjust(wspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 3\n",
    "C = 5\n",
    "fig, axes = plt.subplots(R, C, figsize=(12,8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "misclassified_idx = np.where(Y_pred_classes != Y_true)[0]\n",
    "for i in np.arange(0, R*C):\n",
    "    axes[i].imshow(x_test[misclassified_idx[i]])\n",
    "    axes[i].set_title(\"True: %s \\nPredicted: %s\" % (labels[Y_true[misclassified_idx[i]]], \n",
    "                                                  labels[Y_pred_classes[misclassified_idx[i]]]))\n",
    "    axes[i].axis('off')\n",
    "    plt.subplots_adjust(wspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the most important errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_errors(errors_index, img_errors, pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 10 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 5\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True, figsize=(12,6))\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((32,32,3)))\n",
    "            ax[row,col].set_title(\"Predicted:{}\\nTrue:{}\".\n",
    "                                  format(labels[pred_errors[error]],labels[obs_errors[error]]))\n",
    "            n += 1\n",
    "            ax[row,col].axis('off')\n",
    "            plt.subplots_adjust(wspace=1)\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 10 errors \n",
    "most_important_errors = sorted_dela_errors[-10:]\n",
    "\n",
    "# Show the top 10 errors\n",
    "display_errors(most_important_errors, X_test_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Testing the model with the test images in the test set.\n",
    "Now we can play with our model for some fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test(number):\n",
    "    fig = plt.figure(figsize = (3,3))\n",
    "    test_image = np.expand_dims(x_test[number], axis=0)\n",
    "    test_result = model.predict_classes(test_image)\n",
    "    plt.imshow(x_test[number])\n",
    "    dict_key = test_result[0]\n",
    "    plt.title(\"Predicted: {} \\nTrue Label: {}\".format(labels[dict_key],\n",
    "                                                      labels[Y_true[number]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_test(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the model and weights\n",
    "Note that we need to firstly indicate the directory to save the model and the name of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
